# docs/Metrics_Framework.md
# 产品评估指标框架 (Metrics Framework)

## 1. 指标框架概述

本框架旨在为“土壤固废监测智慧助教平台”定义一套清晰、可衡量的评估指标体系。通过对这些指标的持续追踪和分析，我们可以评估产品是否达到预期目标、用户对产品的接受程度、产品在多大程度上解决了核心痛点，并为后续的产品迭代和优化提供数据支持。

## 2. 北极星指标定义

* **北极星指标:** **教师周活跃教学行为次数**
* **定义:** 专任教师每周使用平台执行核心教学相关功能的总次数。核心教学功能包括：
    * 创建或编辑在线教案
    * 上传或分享教学资源
    * 布置新作业或预习任务
    * 发起课堂互动（如投票、讨论）
    * 批阅学生作业
    * 查看学情分析报告
* **选择依据:** 该指标直接反映了产品对教师核心教学工作的支持程度和教师对平台的依赖度。教师的持续深度使用是平台成功的关键驱动力，能有效带动学生参与，并最终指向教学质量的提升。它是衡量产品是否为教师“赋能”的核心体现。
* **目标 (示例):**
    * MVP上线后第一个月: 平均每位活跃教师每周 > 2次教学行为。
    * MVP上线后三个月: 平均每位活跃教师每周 > 5次教学行为。
    * 长期: 持续提升，并根据用户反馈和功能完善度调整。

## 3. HEART / AARRR 等指标体系详述

我们将结合HEART框架（Happiness, Engagement, Adoption, Retention, Task Success）和AARRR模型（Acquisition, Activation, Retention, Referral, Revenue - 此处Revenue初期不适用，重点关注前几项）来构建更全面的指标体系。

### 3.1 用户愉悦度 (Happiness) - 衡量用户对产品的主观感受

* **指标:**
    * **用户满意度评分 (CSAT):** 通过定期（如每学期末）的问卷调研，请师生对平台的整体满意度、易用性、功能满足度等进行评分 (1-5分制)。
        * *目标:* 教师平均分 > 4.0/5.0，学生平均分 > 3.8/5.0。
    * **净推荐值 (NPS) (远期):** 了解用户向他人推荐本产品的意愿。
    * **定性反馈:** 通过用户访谈、意见反馈收集用户的主观评价和建议。
* **收集方法:** 在线问卷、用户访谈、应用内反馈入口。

### 3.2 用户参与度 (Engagement) - 衡量用户使用产品的深度和频率

* **教师指标:**
    * **核心功能使用频率:**
        * 平均每位教师每周/每月创建/编辑教案次数。
        * 平均每位教师每周/每月上传/分享资源数量。
        * 平均每位教师每周/每月布置作业次数。
        * 平均每位教师每周/每月发起互动次数。
        * 平均每位教师每周/每月批阅作业份数/时长。
    * **在线时长:** 教师用户平均每日/每周在平台上的使用时长。
    * **内容贡献率:** 教师上传或创建的教学资源占总资源的比例（若有共享机制）。
* **学生指标:**
    * **学习资源查看率/完成率:** 学生查看/完成老师发布的教案、资源的比例。
    * **视频观看完成度:** 学生观看教学视频的平均进度。
    * **互动参与率:** 学生参与课堂投票、讨论的比例。
    * **作业提交及时率:** 在截止日期前提交作业的学生比例。
    * **在线时长:** 学生用户平均每日/每周在平台上的学习时长。
* **收集方法:** 后台用户行为数据自动埋点和统计。

### 3.3 用户接受度/采用度 (Adoption) - 衡量新用户开始使用产品的程度

* **教师指标:**
    * **教师注册率/激活率:** 目标课程的专任教师注册并激活账户的比例。
        * *目标:* MVP上线后一个月达到90%。
    * **新教师首次核心功能使用转化率:** 新注册教师在第一周内完成一次核心教学行为（如备课、布置作业）的比例。
* **学生指标:**
    * **学生注册率/激活率:** 目标课程的学生注册并激活账户的比例。
        * *目标:* MVP上线后一个月达到95%。
    * **新学生首次核心功能使用转化率:** 新注册学生在第一周内完成一次核心学习行为（如查看资源、提交作业）的比例。
* **收集方法:** 后台用户账户数据和行为数据统计。

### 3.4 用户留存率 (Retention) - 衡量用户持续使用产品的程度

* **教师指标:**
    * **教师周/月留存率:** 本周/月仍活跃的教师占上一个周期活跃教师的比例。
        * *目标:* 月留存率 > 70%。
    * **核心功能持续使用率:** 连续N周/月使用某核心功能的教师比例。
* **学生指标:**
    * **学生周/月留存率:** 本周/月仍活跃的学生占上一个周期活跃学生的比例。
        * *目标:* 月留存率 > 80% (通常学生因课程需要，留存会较高)。
* **收集方法:** 后台用户行为数据统计。

### 3.5 任务成功率 (Task Success) - 衡量用户能否顺利完成核心任务

* **教师指标:**
    * **备课成功率:** 教师成功创建并发布一个包含多种资源的教案的比例。
    * **作业布置成功率:** 教师成功发布一次作业的比例。
    * **作业批阅效率:** 教师平均批阅一份作业所需时长（与传统方式对比评估）。
* **学生指标:**
    * **作业提交成功率:** 学生尝试提交作业并最终成功的比例。
    * **资源查找效率:** 学生找到所需学习资源的平均耗时或点击次数（通过用户测试评估）。
* **收集方法:** 后台任务完成节点埋点、用户行为路径分析、可用性测试。

## 4. 功能级评估指标

除了上述宏观指标，我们还会关注具体功能的微观指标，以评估其使用情况和效果。

* **教学资源中心:**
    * **资源上传总数与类型分布。**
    * **热门资源排行榜 (按查看/下载次数)。**
    * **资源检索成功率与平均检索时长。**
* **在线备课与授课:**
    * **教案创建数量与平均包含资源数。**
    * **预习任务发布数量与学生平均完成率。**
* **课堂互动工具:**
    * **各类互动工具（投票、讨论）平均每堂课使用次数。**
    * **学生平均参与互动的人数比例。**
* **作业与评估系统:**
    * **不同类型作业的布置频率。**
    * **作业平均批阅时长。**
    * **学生对作业反馈的查看率。**
* **学情分析与反馈:**
    * **教师查看学情报告的频率。**
    * **基于学情数据进行干预的案例数 (通过调研)。**

## 5. 指标监测计划

* **数据收集工具:**
    * **后端埋点:** 通过服务器日志或专门的事件追踪服务记录关键用户行为。
    * **前端埋点 (辅助):** 对于一些前端交互细节，可使用前端埋点方案。
    * **数据库统计:** 定期从业务数据库中提取和汇总数据。
    * **问卷调查工具:** 如问卷星、Google Forms等，用于收集满意度等主观数据。
* **数据可视化与报告:**
    * 使用数据可视化工具（如开源的Metabase、Superset，或商业BI工具）构建Dashboard，实时展示核心指标。
    * 定期生成数据报告 (周报、月报、学期报告)，分析趋势、问题和机会点。
* **监测频率与负责人:**
    * **日常监测 (运营/产品团队):** 每日关注核心指标波动，及时发现异常。
    * **周度回顾 (产品/技术/运营):** 分析周数据，讨论进展和问题。
    * **月度/学期评估 (管理层/核心团队):** 评估KPI达成情况，制定下一步策略。
* **指标迭代:**
    * 随着产品功能的演进和用户需求的变化，定期回顾和调整指标体系，确保其始终与产品目标保持一致。例如，当“在线实验辅助”功能上线后，需要增加相关的任务成功率和参与度指标。

